{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "emotional_scores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9oM2kCjwtlR"
      },
      "source": [
        "# Neural Network for sentiment analysis of press texts\n",
        "\n",
        "### Goal:\n",
        "Creating a Neural Network for sentiment analysis of press materials\n",
        "\n",
        "### Data:\n",
        "Training set contains x news articles from 7 polish news portals. For each of these articles I calculate emotional score based on emotional valuation dictionary from Słowosieć - Polish Wordnet. Later the scores are converted to labels for positively and negatively valued articles, which will serve as a target in supervised learning\n",
        "\n",
        "### Method choice\n",
        "I want to use a Neural Network for this task, as it has higher potential for learning relationship between the words. Learning these relationships, which isn't possible with methods like Logistic Regression etc guarantees that the model will generalize better on texts with different topics than those in training set. "
      ],
      "id": "X9oM2kCjwtlR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reliable-volleyball"
      },
      "source": [
        "# basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# NLP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS \n",
        "import spacy\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.collocations import *"
      ],
      "id": "reliable-volleyball",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hxz9hklqoBC6",
        "outputId": "4942e5eb-324d-41bc-b244-c64eb2e38ede"
      },
      "source": [
        "!pip install\n",
        "!python -m spacy download pl_core_news_lg"
      ],
      "id": "Hxz9hklqoBC6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/62/a98c61912ea57344816dd4886ed71e34d8aeec55b79e5bed05a7c2a1ae52/spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7MB 220kB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (1.19.5)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (20.9)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.11.3)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (54.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (3.7.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (3.7.4.3)\n",
            "Collecting pathy\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0) (0.8.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d5/6c58fc97f3098775e46d8202bf248752e626a8096a0ae9d76aa7c485a09c/spacy_legacy-3.0.1-py2.py3-none-any.whl\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 36.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==3.0) (3.4.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2.10)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=dd4c78f85be77dbf280b0671b4d6674049bc67d30e40496c7a51f417e7bab07a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, pydantic, srsly, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.0 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "catalogue",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-18 09:22:04.297883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting pl-core-news-lg==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.0.0/pl_core_news_lg-3.0.0-py3-none-any.whl (612.6MB)\n",
            "\u001b[K     |████████████████████████████████| 612.6MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from pl-core-news-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (54.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (20.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pl-core-news-lg==3.0.0) (7.1.2)\n",
            "Installing collected packages: pl-core-news-lg\n",
            "Successfully installed pl-core-news-lg-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CEBrpoLvdYp"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "I need to compute the emotional scores for each article in the set. In the emotional dictionary words are written as lemmas, and Logistic Regression(my baseline model) works better with lemmatized words, so I need to lemmatize the articles. Moreover, later in the notebook I need word vectors for creating the embedding matrix for Neural Net. Both of these processes can be done with SpaCy model, which I will use for preprocessing."
      ],
      "id": "9CEBrpoLvdYp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "political-secret",
        "outputId": "6076075d-3dae-42a9-abd4-1dd94fdcfea9"
      },
      "source": [
        "# reading datasets\n",
        "data = {\"gazeta_pl\": pd.read_csv(\"gazeta_pl.csv\",sep=\"^\"),\n",
        "       \"oko_press\": pd.read_csv(\"OKO_press_all.csv\",sep=\"^\"),\n",
        "       \"onet\": pd.read_csv(\"Onet.csv\",sep=\"^\",error_bad_lines=False),\n",
        "       \"wp\":pd.read_csv(\"wp.csv\",sep=\"^\",error_bad_lines=False),\n",
        "       \"wPolityce\":pd.read_csv(\"wPolityce.csv\",sep=\"^\"),\n",
        "       \"niezalezna\":pd.read_csv(\"Niezalezna.csv\",sep=\"^\").rename({\"Content\\r\":\"Content\"},axis=1),\n",
        "       \"krypol\":pd.read_csv(\"krypol.csv\",sep=\"^\")}"
      ],
      "id": "political-secret",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 1195: expected 5 fields, saw 9\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extreme-marketplace"
      },
      "source": [
        "# loading large polish spacy model\n",
        "nlp = spacy.load(\"pl_core_news_lg\")"
      ],
      "id": "extreme-marketplace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "assigned-advocacy",
        "outputId": "9df72ad4-aa0b-4418-c6f1-0887f7d93211"
      },
      "source": [
        "# checking wether model loaded properly\n",
        "nlp(\"ja\")[0].lemma_"
      ],
      "id": "assigned-advocacy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ja'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aggregate-midnight"
      },
      "source": [
        "# loading emotional dictionary\n",
        "emotional_df = pd.read_csv(r\"emotions.csv\")"
      ],
      "id": "aggregate-midnight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electoral-logic"
      },
      "source": [
        "def count_emotional_value(text,emotional_df):\n",
        "    \"\"\"\n",
        "    Function calculates the emotional score of a single text based on evaluations provided by emotional valuation dictionary.\n",
        "    text - lemmatized version of the text to calculate score on\n",
        "    emotional_df - emotional dictionary read as pd.DataFrame\n",
        "    \"\"\"\n",
        "    # variable storing emotional score\n",
        "    emotional_score = 0\n",
        "    # dictionary for translating emotional values into numerical values\n",
        "    values = {\"- m\":-4,\n",
        "             \"- s\":-1,\n",
        "             \"amb\":0,\n",
        "             \"+ s\":1,\n",
        "             \"+ m\":4}\n",
        "             \n",
        "    # iterating over words in texts, checking wether word has emotional valuations, adding values to emotional score\n",
        "    for word in text:\n",
        "        word = str(word)\n",
        "        if word in emotional_df.lemat.tolist():\n",
        "            row = emotional_df[emotional_df[\"lemat\"] == word]\n",
        "            value = values[row.stopien_nacechowania.iloc[0]]\n",
        "            emotional_score += value\n",
        "            \n",
        "    return emotional_score"
      ],
      "id": "electoral-logic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greek-stranger"
      },
      "source": [
        "def preprocess_data(data,nlp,emotional_df):\n",
        "    \n",
        "    # process texts - saving columns with spacy docs and with lemmatized texts as lists of words\n",
        "    data[\"processed\"] = [nlp(x) for x in data.Content.astype(\"str\")]\n",
        "    data[\"lemmatized\"] = [[x.lemma_ for x in text] for text in data.processed]\n",
        "        \n",
        "    \n",
        "    \n",
        "    # computing emotional values of texts\n",
        "    data[\"emotional_value\"] = [count_emotional_value(x,emotional_df) for x in data.lemmatized]\n",
        "    \n",
        "    \n",
        "    return data"
      ],
      "id": "greek-stranger",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufficient-jason"
      },
      "source": [
        "# preprocessing data from each data frame\n",
        "\n",
        "for key in data.keys():\n",
        "        data[key] = preprocess_data(data[key],nlp,emotional_df)"
      ],
      "id": "sufficient-jason",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rolled-opportunity"
      },
      "source": [
        "# checking distributions of positive and negative class\n",
        "for key in data.keys():\n",
        "    # print amounts of articles in positive and negative class\n",
        "    print(\"Positive\",data[key][data[key].emotional_value > 0].Title.count(),\n",
        "          \"Negative\",data[key][data[key].emotional_value < 0].Title.count())\n",
        "    \n",
        "    # plot histogram to see the distribution of values\n",
        "    plt.hist(data[key].emotional_value)\n",
        "    plt.show()"
      ],
      "id": "rolled-opportunity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "studied-england"
      },
      "source": [
        "# Data preparation for Neural Network\n",
        "\n",
        "In the next steps I prepare the data to enter Neural Network - Train test split, Tokenization, Integer Encoding, creating starting embedding matrix, padding."
      ],
      "id": "studied-england"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-korea"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "# tensorflow and keras\n",
        "import keras\n",
        "from keras import regularizers, optimizers\n",
        "from keras.layers.experimental.preprocessing import TextVectorization\n",
        "from keras.layers import Activation, Bidirectional,Embedding, Dense, Dropout, Input, LSTM,MaxPooling1D,Flatten,SpatialDropout1D,BatchNormalization, Conv1D\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import History,EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import one_hot"
      ],
      "id": "crude-korea",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metric-isaac"
      },
      "source": [
        "# creating lerning set\n",
        "\n",
        "learning_set = pd.DataFrame({\"Emotion\":[], \"Text\":[]})\n",
        "\n",
        "for key in data.keys():\n",
        "    # for each dataframe take emotional values and text, assign to new DF, append to learning set\n",
        "    \n",
        "    frame = pd.DataFrame({\"Emotion\":data[key].emotional_value,\"Text\":data[key].processed})\n",
        "    learning_set = learning_set.append(frame,ignore_index=True)"
      ],
      "id": "metric-isaac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collective-walnut"
      },
      "source": [
        "# creating learning set for logistic regression - lemmatized texs as strings\n",
        "learning_set[\"Logistic\"] = [str([x.lemma_ for x in text]).replace(\"'\",\"\").replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\\\n",
        "                            for text in learning_set.Text]"
      ],
      "id": "collective-walnut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M64iIu2PVyw"
      },
      "source": [
        "learning_set = pd.read_csv(\"learning_set.csv\",sep=\"^\")"
      ],
      "id": "_M64iIu2PVyw",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "removed-flash"
      },
      "source": [
        "# drop NaNs from learning set\n",
        "learning_set.dropna(inplace=True)"
      ],
      "id": "removed-flash",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funky-marketing"
      },
      "source": [
        "# prepare target and features variable for Neural Network and Logistic Regression\n",
        "\n",
        "y = pd.Series([0 if x<0 else 1 for x in learning_set[\"Emotion\"]])\n",
        "y_log = pd.Series([0 if x<0 else 1 for x in learning_set[\"Emotion\"]])\n",
        "X_log = pd.Series(learning_set[\"Logistic\"])\n",
        "X = pd.Series(learning_set[\"Text\"])"
      ],
      "id": "funky-marketing",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vocational-gilbert"
      },
      "source": [
        "#  train test split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "X_train_l,X_test_l,y_train_l,y_test_l = train_test_split(X_log,y_log,test_size=0.2)"
      ],
      "id": "vocational-gilbert",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corrected-helen"
      },
      "source": [
        "Baseline model - logistic regression"
      ],
      "id": "corrected-helen"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "connected-omega",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a1a681e-f0ee-4243-d5a1-a158b6ed18a0"
      },
      "source": [
        "# fit the data on simple logistic regression to check wether the set is classifiable\n",
        "\n",
        "mdl = make_pipeline(CountVectorizer(),LogisticRegression(solver=\"liblinear\"))\n",
        "                    \n",
        "mdl.fit(X_train_l,y_train_l)\n",
        "mdl.score(X_test_l,y_test_l)"
      ],
      "id": "connected-omega",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8435434111943587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "continuous-pasta"
      },
      "source": [
        "# prepare train and test variables for tokenizing - make sure that every element is a string\n",
        "\n",
        "X_train_for_tokenizer = [[str(x) for x in y] for y in X_train] \n",
        "X_test_for_tokenizer = [[str(x) for x in y] for y in X_test] \n",
        "\n"
      ],
      "id": "continuous-pasta",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLNwLknpSljX",
        "outputId": "81fc5d29-8050-49ab-d6a3-669f0b74e3d7"
      },
      "source": [
        "X_train[:10]"
      ],
      "id": "yLNwLknpSljX",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9299     ['Pytany', 'w', 'programie', 'Michała', 'Racho...\n",
              "5240     ['Kolejna', 'partia', 'szczepionek', 'na', 'CO...\n",
              "5025     ['Dopiero', 'zaczynało', 'świtać', ',', 'gdy',...\n",
              "403      ['Minionej', 'doby', 'odnotowano', '7', '038',...\n",
              "10130    ['81', '-', 'letnia', 'mieszkanka', 'Olsztyna'...\n",
              "378      ['Kancelaria', 'Prezydenta', 'poinformowała', ...\n",
              "8935     ['Jeszcze', 'do', '25', 'lutego', 'uczniowie',...\n",
              "319      ['Amerykańska', 'agencja', 'kosmiczna', 'NASA'...\n",
              "3976     ['Ojciec', 'Święty', 'nawiązał', 'do', '90', '...\n",
              "10830    ['Politycy', ',', 'publicyści', 'oraz', 'dyplo...\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demanding-prescription",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d262a488-9b9d-4b41-e35b-9a8645b409c1"
      },
      "source": [
        "# median length of textsw\n",
        "np.median([len(x) for x in X_train])"
      ],
      "id": "demanding-prescription",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2887.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "included-conservative"
      },
      "source": [
        "# tokenizer with custom filter - does not filter out comas, stops, ? and !, as they can carry important insights\n",
        "t = Tokenizer(num_words=50000,lower=False,filters='#$%&+-/<=>@[\\\\]^_`{|}~\\t\\n')\n",
        "t.fit_on_texts(X_train)\n",
        "\n",
        "# vocab size for embedding martix\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(X_train)\n",
        "encoded_test = t.texts_to_sequences(X_test)\n",
        "\n",
        "# padding texts \n",
        "max_length = 1000\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
        "\n",
        "    \n"
      ],
      "id": "included-conservative",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_HCJhGCScUP",
        "outputId": "56caf1a6-98b6-4bab-8ce3-21c078d6fa2e"
      },
      "source": [
        "padded_docs[0]"
      ],
      "id": "k_HCJhGCScUP",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1606,     3,   863,  1775, 11104,    13,     4,     5, 10257,\n",
              "          13,    16,    14,     1,    42,   546,  6051,  5902,     7,\n",
              "         415,   824,     8,   121,     1, 10025,   527,     1, 13247,\n",
              "           1,   740,  4517, 10792,   291,     1,    10,     3,  3548,\n",
              "       22802,     1, 22803,     1,    15,    38,    11,   610,  6403,\n",
              "       25853,     2,     4,     5,  4677,    12,    58,   658,     2,\n",
              "         757,    31,  1042,  3121,  2057,  1594,     3,   666,  1419,\n",
              "           3,   273,    59,     2,     6,    14,    12,    15,  3121,\n",
              "        1268,     4,     5,   761,     2,    18,   334,  1078,  1024,\n",
              "           6,  1274,   730,  7778,  6404,   509,  1326,    25,  1326,\n",
              "           4,     5,   659,  1021,  2039,     3,   736,    11,  9320,\n",
              "        6590,  5961,    27,   357,   263,     1,   159,   509,   393,\n",
              "           1,  2907,     1,   546,     6,   567,   123,   437,   919,\n",
              "         527,     2,   143,  6793,  5025, 46875,  7505,  7240, 25854,\n",
              "          30,  4741,    69, 14779,     3,  5025,  1632,   121,     1,\n",
              "       15396,   104,  2028,     1,    10,  3549, 27809,  4256,   117,\n",
              "          69,   290,     4,     5,     4,     5,     4,   191,    90,\n",
              "          91,   192,     4,     5,   193,    17,    92,    24,    93,\n",
              "          17,    92,    24,     4,     5,     4,     5,    90,    91,\n",
              "         187,     4,     5,     4,     5,    94,     4,     5,   194,\n",
              "          17,   195,    24,   196,    17,     4,     5,   197,    24,\n",
              "          94,     4,     5,   198,    17,   199,    24,   188,     4,\n",
              "           5,    93,    17,   200,    24,     4,     5,   201,    17,\n",
              "          60,   161,    60,    60,    24,     4,     5,     4,     5,\n",
              "           4,     5,     4,     5,     4,   202, 11747,  1043, 12449,\n",
              "          16,   509,  1326,    43,    18,   702, 21515,     6, 20336,\n",
              "       21516,     2,     3,   863,  1775, 11104,    13,     4,     5,\n",
              "       10257,    13,    16,    14,     1,    42,   546,  6051,  5902,\n",
              "           7,   415,   824,     8,   121,     1, 10025,   527,     1,\n",
              "       13247,     1,   740,  4517, 10792,   291,     1,    10,     3,\n",
              "        3548, 22802,     1, 22803,     1,    15,    38,    11,   610,\n",
              "        6403, 25853,     2,  4677,    12,    58,   658,     2,  3881,\n",
              "         127,    16,    28,  2388,   477,   582,  6405,     2,   757,\n",
              "          31,  1042,  3121,  2057,  1594,     3,   666,  1419,     3,\n",
              "         273,    59,     2,     6,    14,    12,    15,  3121,  1268,\n",
              "           4,     5,   761,     2, 25855,    11,   510,   203,    15,\n",
              "       40932, 11748,     2, 10793,  1143,    33, 13729,    11,  1044,\n",
              "           1,   146,   210, 36407,     2,   139,  2214,     1,   126,\n",
              "        1424,    36,     3,   232, 40933,  3761,  2351,     2,   477,\n",
              "           1,    12,    35,    38,  1469,    16,   610,  2057,   824,\n",
              "           4,     5,   134,     2,     4,    99,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujKyOHjJ1Isl"
      },
      "source": [
        "# Creating architecture and training model\n",
        "\n",
        "I use architecture based on LSTM's, as they can work with text data as sequences and they capture relationship between words best. I use LSTM's instead of simple RNN's because i want the model to consider long term relationships between words, which simple RNN's do worse. \n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "ujKyOHjJ1Isl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secret-radius",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfb727c-afa9-4aaa-acc8-910b676ff7a5"
      },
      "source": [
        "history = History()\n",
        "\n",
        "# Creating \n",
        "model = Sequential()\n",
        "model.add(Embedding(\n",
        "    50000,\n",
        "    100,\n",
        "    input_length = max_length,\n",
        "    trainable=True))\n",
        "model.add(Bidirectional(LSTM(32,return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.65))\n",
        "\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.65))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "model.summary()"
      ],
      "id": "secret-radius",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 1000, 100)         5000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1000, 64)          34048     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 1000, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 1000, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1000, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1000, 32)          2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 1000, 32)          128       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1000, 1)           33        \n",
            "=================================================================\n",
            "Total params: 5,036,545\n",
            "Trainable params: 5,036,353\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "honey-aerospace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "2a59da82-37c0-4c68-cf14-6e6d9d51d6de"
      },
      "source": [
        "Adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=Adam, metrics=[\"accuracy\"])\n",
        "save_best_model = ModelCheckpoint(\"sentiment.h5\",save_best_only=True)\n",
        "\n",
        "model.fit(padded_docs,y_train,validation_split=0.2,batch_size=32,epochs=15,callbacks=[history,save_best_model])\n"
      ],
      "id": "honey-aerospace",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "227/227 [==============================] - 30s 116ms/step - loss: 0.8243 - accuracy: 0.5050 - val_loss: 0.6435 - val_accuracy: 0.6942\n",
            "Epoch 2/15\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.6551 - accuracy: 0.6178 - val_loss: 0.6220 - val_accuracy: 0.6554\n",
            "Epoch 3/15\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.5469 - accuracy: 0.7442 - val_loss: 0.6292 - val_accuracy: 0.6617\n",
            "Epoch 4/15\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.4859 - accuracy: 0.7960 - val_loss: 0.6690 - val_accuracy: 0.6660\n",
            "Epoch 5/15\n",
            "227/227 [==============================] - 25s 110ms/step - loss: 0.4636 - accuracy: 0.8222 - val_loss: 0.7916 - val_accuracy: 0.6031\n",
            "Epoch 6/15\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.4531 - accuracy: 0.8277 - val_loss: 0.6338 - val_accuracy: 0.6764\n",
            "Epoch 7/15\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.3674 - accuracy: 0.8777 - val_loss: 0.7307 - val_accuracy: 0.7059\n",
            "Epoch 8/15\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.2531 - accuracy: 0.9328 - val_loss: 0.8817 - val_accuracy: 0.7153\n",
            "Epoch 9/15\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.2182 - accuracy: 0.9456 - val_loss: 0.9674 - val_accuracy: 0.7015\n",
            "Epoch 10/15\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.2253 - accuracy: 0.9477 - val_loss: 0.8563 - val_accuracy: 0.7098\n",
            "Epoch 11/15\n",
            "227/227 [==============================] - 25s 111ms/step - loss: 0.2244 - accuracy: 0.9444 - val_loss: 0.8452 - val_accuracy: 0.7059\n",
            "Epoch 12/15\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.2012 - accuracy: 0.9362 - val_loss: 0.8469 - val_accuracy: 0.5994\n",
            "Epoch 13/15\n",
            "227/227 [==============================] - 26s 113ms/step - loss: 0.4597 - accuracy: 0.7429 - val_loss: 0.8593 - val_accuracy: 0.7117\n",
            "Epoch 14/15\n",
            "227/227 [==============================] - 25s 112ms/step - loss: 0.2316 - accuracy: 0.9180 - val_loss: 1.2167 - val_accuracy: 0.6486\n",
            "Epoch 15/15\n",
            " 86/227 [==========>...................] - ETA: 14s - loss: 0.3044 - accuracy: 0.8860"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a88137d87117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_best_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wireless-religion"
      },
      "source": [
        ""
      ],
      "id": "wireless-religion",
      "execution_count": null,
      "outputs": []
    }
  ]
}